{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of toy example.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YINOa3pt5PIa",
        "x4Gfk6LO5q-1",
        "4pyL_0Jf6A-3",
        "MjmJUpnJxW8X",
        "q0oRc9Nf-d4O",
        "RJzZhp-Jch4d",
        "ppM3T_uaeI6D",
        "D9UWT7WEeVPb",
        "BvFy_lSL1ro2",
        "0pXY_E0BOovS",
        "7V8ksCbxSPTA",
        "AnC61B1MKAOA",
        "YTCfQqab6JD7",
        "k_BCe3uu8ESN",
        "qYc34RaobUws"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YINOa3pt5PIa"
      },
      "source": [
        "## Imports and Utility Functions\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoimdN6c4KfH"
      },
      "source": [
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "# tf.reset_default_graph()\n",
        "# !pip install tf-nightly\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(1234)\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from collections import defaultdict\n",
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qV3LlRV5NZ7"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self, alpha=0.9):\n",
        "        self.reset()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        if not hasattr(self, \"moving_avg\"):\n",
        "            self.moving_avg = val\n",
        "        else:\n",
        "            self.moving_avg = self.moving_avg * self.alpha + val * (1 - self.alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlfoNWSa6YzJ"
      },
      "source": [
        "## Objective Function for Maximimization\n",
        "\n",
        "---\n",
        "\n",
        "$ (1/D) \\mathbb{E}_{q_\\eta(b)}\\left[\\|b - t\\|^2\\right], \\quad b^1,b^2,\\dots,b^K \\sim q_{\\eta}(b) := \\mathrm{Bernoulli}(\\theta), \\quad \\theta = 1 / (1 + e^{-\\eta}), \\quad \\eta \\in \\mathbb{R}^D$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSlHA_kG4pdZ"
      },
      "source": [
        "K = 2\n",
        "D = 200\n",
        "t = 0.499\n",
        "target = np.array([[t for i in range(D)]], dtype=np.float64)\n",
        "W = tf.constant(np.random.normal(0.0, 0.01, size=[D, D]).astype(np.float64))\n",
        "print(\"Target is {}\".format(target))\n",
        "\n",
        "# @tf.function\n",
        "def exact_grad(theta):\n",
        "    return (1. - theta) * theta * ((1 - target[0, 0])**2 - target[0, 0]**2)\n",
        "\n",
        "# @tf.function\n",
        "def loss_func(b, t):\n",
        "    return tf.reduce_mean((b - t)**2, axis=1) # - 0.1*tf.reduce_mean(t*tf.math.log(t), axis=1)\n",
        "\n",
        "# @tf.function\n",
        "def safe_log(x, eps=1e-8):\n",
        "    return tf.math.log(tf.clip_by_value(x, eps, 1.0))\n",
        "\n",
        "# @tf.function\n",
        "def reparameterize_q(eta, noise):\n",
        "    # noise: uniform [0, 1]\n",
        "    return eta + safe_log(noise) - safe_log(1 - noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mlfg6Z25njK"
      },
      "source": [
        "## Gradient Estimators\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4Gfk6LO5q-1"
      },
      "source": [
        "### Standard REINOFRCE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pELtBljR4lex"
      },
      "source": [
        "# @tf.function\n",
        "def reinforce(eta, theta, b, K, baseline=None, baseline_ema=None, dry_run=None):\n",
        "    # loss function evaluations\n",
        "    f_b = loss_func(b, target)\n",
        "    loss = tf.reduce_mean(f_b)\n",
        "\n",
        "    fb_moving_avg = baseline_ema.average(baseline)\n",
        "    fb_keep = f_b\n",
        "    if fb_moving_avg is not None:\n",
        "        f_b = f_b - 0.0*fb_moving_avg\n",
        "\n",
        "    # f_b: [K]\n",
        "    # b: [K, D]\n",
        "    # eta: [1, D]\n",
        "    # dlog_q: [K, D]\n",
        "    dlog_q = b - theta\n",
        "    eta_grads = tf.reduce_mean(f_b[:, None] * dlog_q, axis=0, keepdims=True)\n",
        "\n",
        "    if not dry_run:\n",
        "        baseline.assign(tf.reduce_mean(fb_keep))\n",
        "        baseline_ema.apply([baseline])\n",
        "\n",
        "    return loss, eta_grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pyL_0Jf6A-3"
      },
      "source": [
        "### REINFORCE Leave-One-Out (RLOO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjAF-oo04nVd"
      },
      "source": [
        "# @tf.function\n",
        "def reinforce_loo(eta, theta, b, K):\n",
        "    if K < 2:\n",
        "        raise NotImplementedError(\"Leave-one-out requires K > 1.\")\n",
        "    # loss function evaluations\n",
        "    f_b = loss_func(b, target)\n",
        "    loss = tf.reduce_mean(f_b)\n",
        "    # f_b: [K]\n",
        "    # f_not_k: [K]\n",
        "    f_not_k = tf.reduce_sum(f_b, axis=0) - f_b\n",
        "    fk_minus_avg_f_not_k = f_b - f_not_k / (K - 1)\n",
        "    # b: [K, D]\n",
        "    # eta: [1, D]\n",
        "    # dlog_q: [K, D]\n",
        "    dlog_q = b - theta\n",
        "    eta_grads = tf.reduce_mean(fk_minus_avg_f_not_k[:, None] * dlog_q, axis=0, keepdims=True)\n",
        "    return loss, eta_grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqCGKsUCF_OM"
      },
      "source": [
        "### MuProp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4-YFfnZF_97"
      },
      "source": [
        "# @tf.function\n",
        "def muprop(eta, theta, b, K, eta_mu=None, alpha=None):\n",
        "    def _get_df_mu(mu):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(mu)\n",
        "            # f_mu: [1]\n",
        "            f_mu = loss_func(mu, target)\n",
        "            f_mu_sum = tf.reduce_sum(f_mu)\n",
        "        # df_mu: [1, D]\n",
        "        df_mu = tape.gradient(f_mu_sum, mu)\n",
        "        return f_mu, df_mu\n",
        "\n",
        "    # loss function evaluations\n",
        "    f_b = loss_func(b, target)\n",
        "    loss = tf.reduce_mean(f_b)\n",
        "    mu = theta\n",
        "    f_mu, df_mu = _get_df_mu(theta)\n",
        "    correction = df_mu * theta * (1. - theta)  \n",
        "    # f_b: [K]\n",
        "    # b: [K, D]\n",
        "    # eta: [1, D]\n",
        "    # dlog_q: [K, D]\n",
        "    dlog_q = b - theta\n",
        "    # baseline: [K]\n",
        "    baseline = f_mu + tf.reduce_sum(df_mu * (b - mu), axis=-1) \n",
        "    eta_grads = tf.reduce_mean(\n",
        "        (f_b - baseline)[:, None] * dlog_q, axis=0, keepdims=True) + correction\n",
        "    return loss, eta_grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0oRc9Nf-d4O"
      },
      "source": [
        "### DisARM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SNI_iic-cQo"
      },
      "source": [
        "def disarm(eta, theta, u, b, K):\n",
        "    if K % 2 != 0:\n",
        "        raise RuntimeError(\"DisARM requires K % 2 = 0.\")\n",
        "    # u: [K // 2, D]\n",
        "    u = u[:(K // 2), :]\n",
        "    # b: [K // 2, D]\n",
        "    b = b[:(K // 2), :]\n",
        "    # b_anti: [K // 2, D]\n",
        "    b_anti = tf.cast(u < theta, tf.float64)\n",
        "    # f_b: [K // 2]\n",
        "    f_b = loss_func(b, target)\n",
        "    # f_b_anti: [K // 2]\n",
        "    f_b_anti = loss_func(b_anti, target)\n",
        "    loss = 0.5 * tf.reduce_mean(f_b) + 0.5 * tf.reduce_mean(f_b_anti)\n",
        "    # dlog_q: [K // 2, D]\n",
        "    dlog_q = (-1.)**b_anti * tf.cast(b != b_anti, tf.float64) * tf.sigmoid(tf.abs(eta))\n",
        "    eta_grads = 0.5 * tf.reduce_mean((f_b - f_b_anti)[:, None] * dlog_q, axis=0, keepdims=True)\n",
        "    return loss, eta_grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhWJ943fRqws"
      },
      "source": [
        "### Double Control Variates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SP8_d77H-jt"
      },
      "source": [
        "# @tf.function\n",
        "def double_control_variate(eta, theta, b, K, alpha):\n",
        "    if K < 2:\n",
        "        raise NotImplementedError(\"Leave-one-out requires K > 1.\")\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(b)\n",
        "        # f_b: [K]\n",
        "        f_b = loss_func(b, target)\n",
        "        loss = tf.reduce_mean(f_b)\n",
        "        f_b_sum = tf.reduce_sum(f_b)\n",
        "    # grad_b: [K, D]\n",
        "    grad_b = tape.gradient(f_b_sum, b)\n",
        "\n",
        "    b1 = alpha*tf.reduce_sum(grad_b[1,:]*(b[0,:] - theta)) \n",
        "    c1 = alpha*tf.reduce_sum(grad_b[0,:]*(b[1,:] - theta)) \n",
        "    dlog_q = b - theta\n",
        "    grad_avg = 0.5*(grad_b[1,:]+grad_b[0,:])\n",
        "    global_corr = alpha*grad_avg*(theta*(1. - theta)) \n",
        "    diffs = f_b[0] + b1 - (f_b[1] + c1)\n",
        "    eta_grads = 0.5 * ( diffs * dlog_q[0,:] - diffs * dlog_q[1,:] ) - global_corr\n",
        "    return loss, eta_grads\n",
        "\n",
        "# The following two auxiliary methods are only used to produce figure 5 in the appendix.\n",
        "def double_control_variate_onlybxk(eta, theta, b, K, alpha):\n",
        "    if K < 2:\n",
        "        raise NotImplementedError(\"Leave-one-out requires K > 1.\")\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(b)\n",
        "        # f_b: [K]\n",
        "        f_b = loss_func(b, target)\n",
        "        loss = tf.reduce_mean(f_b)\n",
        "        f_b_sum = tf.reduce_sum(f_b)\n",
        "    # grad_b: [K, D]\n",
        "    grad_b = tape.gradient(f_b_sum, b)\n",
        "\n",
        "    b0 = alpha*tf.reduce_sum(grad_b[1,:]*(b[0,:] - theta))  # + beta*tf.reduce_mean((b[0,:]-theta)**2)\n",
        "    b1 = alpha*tf.reduce_sum(grad_b[0,:]*(b[1,:] - theta))  # + beta*tf.reduce_mean((b[1,:]-theta)**2)\n",
        "    dlog_q = b - theta\n",
        "    grad_avg = 0.5*(grad_b[0,:]+grad_b[1,:])\n",
        "    global_corr = alpha*grad_avg*(theta*(1. - theta))  #+ (beta / D)*(theta*(1.-theta)*(1. - 2.*theta))\n",
        "    diffs0 = f_b[0] + b0 - f_b[1]\n",
        "    diffs1 = f_b[1] + b1 - f_b[0] \n",
        "    eta_grads = 0.5 * ( diffs0 * dlog_q[0,:] + diffs1 * dlog_q[1,:] ) - global_corr\n",
        "    return loss, eta_grads\n",
        "\n",
        "def double_control_variate_onlybxj(eta, theta, b, K, alpha):\n",
        "    if K < 2:\n",
        "        raise NotImplementedError(\"Leave-one-out requires K > 1.\")\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(b)\n",
        "        # f_b: [K]\n",
        "        f_b = loss_func(b, target)\n",
        "        loss = tf.reduce_mean(f_b)\n",
        "        f_b_sum = tf.reduce_sum(f_b)\n",
        "    # grad_b: [K, D]\n",
        "    grad_b = tape.gradient(f_b_sum, b)\n",
        "\n",
        "    b0 = alpha*tf.reduce_sum(grad_b[1,:]*(b[0,:] - theta))  # + beta*tf.reduce_mean((b[0,:]-theta)**2)\n",
        "    b1 = alpha*tf.reduce_sum(grad_b[0,:]*(b[1,:] - theta))  # + beta*tf.reduce_mean((b[1,:]-theta)**2)\n",
        "    dlog_q = b - theta\n",
        "    diffs0 = f_b[0] - (f_b[1] + b1)\n",
        "    diffs1 = f_b[1] - (f_b[0] + b0)\n",
        "    grad_avg = 0.5*(grad_b[0,:]+grad_b[1,:])\n",
        "    global_corr = 0.*grad_avg*(theta*(1. - theta))\n",
        "    eta_grads = 0.5 * ( diffs0 * dlog_q[0,:] + diffs1 * dlog_q[1,:] ) - global_corr\n",
        "    return loss, eta_grads         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V8ksCbxSPTA"
      },
      "source": [
        "### Exact Mean Baseline (R*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ6SXNle25WX"
      },
      "source": [
        "# @tf.function\n",
        "def exact_mean_control_variate(eta, theta, b, K):\n",
        "    if K < 2:\n",
        "        raise NotImplementedError(\"Leave-one-out requires K > 1.\")\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(b)\n",
        "        # f_b: [K]\n",
        "        f_b = loss_func(b, target)\n",
        "        loss = tf.reduce_mean(f_b)\n",
        "        f_b_sum = tf.reduce_sum(f_b)\n",
        "    loss = tf.reduce_mean(f_b)\n",
        "    st_baseline = tf.reduce_mean(theta*((1.0 - target)**2) + (1-theta)*(target**2) ) # exact mean of f\n",
        "    # dlog_q: [K, D]\n",
        "    dlog_q = b - theta\n",
        "    eta_grads = tf.reduce_mean((f_b[:, None] - st_baseline) * dlog_q, axis=0, keepdims=True)\n",
        "    return loss, eta_grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFx3PvIJEm-V"
      },
      "source": [
        "## Training\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjyOf_3l4tZW"
      },
      "source": [
        "def main(estimator=\"reinforce_loo\", eps=-1, lr=0.01, iters=2000):\n",
        "    @tf.function \n",
        "    def train_one_step(eta, inf_opt, hyper_opt, estimator=\"reinforce_loo\", eta_mu=None, baseline=None, baseline_ema=None, alpha=None, control_nn=None, dry_run=False):\n",
        "        theta = tf.sigmoid(eta)\n",
        "\n",
        "        print(tf.reduce_mean(theta))\n",
        "        u = tf.random.uniform([K, D], dtype=tf.float64)\n",
        "        z = reparameterize_q(eta, u)  # z(u)\n",
        "        b = tf.cast(tf.stop_gradient(z > 0), dtype=tf.float64)\n",
        "\n",
        "        # Methods\n",
        "        if estimator == \"reinforce_loo\":\n",
        "            loss, eta_grads = reinforce_loo(eta, theta, b, K)\n",
        "        elif estimator == \"reinforce\":\n",
        "            loss, eta_grads = reinforce(eta, theta, b, K, baseline, baseline_ema, dry_run=dry_run)\n",
        "        elif estimator == \"muprop\":\n",
        "            loss, eta_grads = muprop(eta, theta, b, K, eta_mu=None)\n",
        "        elif estimator == \"disarm\":\n",
        "            loss, eta_grads = disarm(eta, theta, u, b, K)\n",
        "        elif estimator == \"exact_mean_control_variate\":\n",
        "            loss, eta_grads = exact_mean_control_variate(eta, theta, b, K)   \n",
        "        elif estimator == \"double_control_variate\":\n",
        "            with tf.GradientTape() as tape:\n",
        "                tape.watch(alpha)\n",
        "                loss, eta_grads = double_control_variate(eta, theta, b, K, alpha)\n",
        "                variance_loss = tf.reduce_mean(tf.square(eta_grads))\n",
        "            if not dry_run:\n",
        "                alpha_grads = tape.gradient(variance_loss, alpha)\n",
        "                hyper_opt.apply_gradients([(alpha_grads, alpha)])                \n",
        "        elif estimator == \"double_control_variate_onlybxk\":\n",
        "            with tf.GradientTape() as tape:\n",
        "                tape.watch(alpha)\n",
        "                loss, eta_grads = double_control_variate_onlybxk(eta, theta, b, K, alpha)\n",
        "                variance_loss = tf.reduce_mean(tf.square(eta_grads))\n",
        "            if not dry_run:\n",
        "                alpha_grads = tape.gradient(variance_loss, alpha)\n",
        "                hyper_opt.apply_gradients([(alpha_grads, alpha)])\n",
        "        elif estimator == \"double_control_variate_onlybxj\":\n",
        "            with tf.GradientTape() as tape:\n",
        "                tape.watch(alpha)\n",
        "                loss, eta_grads = double_control_variate_onlybxj(eta, theta, b, K, alpha)\n",
        "                variance_loss = tf.reduce_mean(tf.square(eta_grads))\n",
        "            if not dry_run:\n",
        "                alpha_grads = tape.gradient(variance_loss, alpha)\n",
        "                hyper_opt.apply_gradients([(alpha_grads, alpha)])                  \n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        if dry_run:\n",
        "            return eta_grads\n",
        "\n",
        "        exact_grads = exact_grad(theta)\n",
        "        inf_opt.apply_gradients([(-eta_grads, eta)])\n",
        "\n",
        "        return loss, theta, eta_grads, exact_grads\n",
        "    \n",
        "    eta = tf.Variable(\n",
        "        [[0.0 for i in range(D)]],\n",
        "        trainable=True,\n",
        "        name='eta',\n",
        "        dtype=tf.float64\n",
        "    )\n",
        "    eta_mu = tf.Variable(\n",
        "        [[0.0 for i in range(D)]],\n",
        "        trainable=True,\n",
        "        name='eta_mu',\n",
        "        dtype=tf.float64\n",
        "    )\n",
        "    alpha = tf.Variable(\n",
        "        0.,\n",
        "        trainable=True,\n",
        "        name=\"alpha\",\n",
        "        dtype=tf.float64\n",
        "    )\n",
        "   \n",
        "    control_nn = tf.keras.Sequential()\n",
        "    control_nn.add(\n",
        "       tf.keras.layers.Dense(137, activation=tf.keras.layers.LeakyReLU(alpha=0.3), dtype=tf.float64))\n",
        "    control_nn.add(tf.keras.layers.Dense(1, dtype=tf.float64))\n",
        "    baseline = tf.Variable(initial_value=0., dtype=tf.float64)\n",
        "    baseline_ema = tf.train.ExponentialMovingAverage(0.6)\n",
        "    inf_opt = tf.keras.optimizers.RMSprop(0.01)\n",
        "    hyper_opt = tf.keras.optimizers.RMSprop(0.0005) \n",
        "\n",
        "    meters = [AverageMeter(alpha=0.6) for i in range(4)]\n",
        "    thetas = []\n",
        "    losses = []\n",
        "    alphas = []\n",
        "    variances = defaultdict(list)\n",
        "    epsilons = []\n",
        "    eta_grad_vals = []\n",
        "    exact_grad_vals = []\n",
        "    compare_basket = []\n",
        "    if len(compare_basket) == 0:\n",
        "        compare_basket.append(estimator)\n",
        "    var_meters = {est: AverageMeter(alpha=0) for est in compare_basket}\n",
        "\n",
        "    for i in tqdm(range(iters)):\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            loss_value, theta_value, eta_grad_val, exact_grad_val = train_one_step(\n",
        "                eta, inf_opt, hyper_opt, estimator=estimator, eta_mu=eta_mu, baseline=baseline, baseline_ema=baseline_ema, alpha=alpha, control_nn=control_nn)\n",
        "\n",
        "            target_theta = tf.cast(tf.sigmoid(eta) > 0.5, tf.float64)\n",
        "            tv = tf.reduce_mean(tf.abs(theta_value)).numpy()\n",
        "            meters[0].update(tv)\n",
        "            meters[1].update(loss_value.numpy())\n",
        "            thetas.append(meters[0].moving_avg)\n",
        "            losses.append(meters[1].moving_avg)\n",
        "            eta_grad_vals.append(eta_grad_val.numpy()[0,0])\n",
        "            exact_grad_vals.append(exact_grad_val.numpy()[0,0])\n",
        "\n",
        "            meters[2].update(alpha.numpy())\n",
        "            alphas.append(meters[2].moving_avg)\n",
        "\n",
        "            for est in compare_basket:\n",
        "                grads = []\n",
        "                for i in range(2000):\n",
        "                    g = train_one_step(eta, inf_opt, hyper_opt, estimator=est, eta_mu=eta_mu, baseline=baseline, baseline_ema=baseline_ema, alpha=alpha, control_nn=control_nn, dry_run=True)\n",
        "                    grads.append(g.numpy()[0])\n",
        "                m, v = np.mean(grads), np.mean(np.std(grads, axis=0)**2)\n",
        "                var_meters[est].update(v)\n",
        "\n",
        "                variances[est].append(var_meters[est].moving_avg)\n",
        "        else:\n",
        "            train_one_step(eta, inf_opt, hyper_opt, estimator=estimator, eta_mu=eta_mu, baseline=baseline, baseline_ema=baseline_ema, alpha=alpha, control_nn=control_nn)\n",
        "\n",
        "    return tv, thetas, losses, variances, epsilons, eta_grad_vals, exact_grad_vals, alphas\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IdxLf08Ktaj"
      },
      "source": [
        "For damping in Fisher control variates, search over $\\epsilon$ in $10^{-6}$ to $10^3$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3ctxUb64wsI"
      },
      "source": [
        "eps_list = []\n",
        "results = {}\n",
        "for method in [\"muprop\", \"reinforce\", \"reinforce_loo\", \"exact_mean_control_variate\", \"double_control_variate\", \"disarm\"]:\n",
        "    print(\"\\n{}...\\n\".format(method))\n",
        "    results[method] = main(method)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0qgDLTqEwgm"
      },
      "source": [
        "## Plots\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jNoaG9VE28f"
      },
      "source": [
        "### Variance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhkIPyr15D_S"
      },
      "source": [
        "display_eps_list = []\n",
        "rc('text', usetex=True)\n",
        "rc('font', family='serif')\n",
        "font_size = 18\n",
        "matplotlib.style.use('default')\n",
        "plt.rc('font', size=font_size)         # controls default text sizes\n",
        "plt.rc('axes', titlesize=font_size)    # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=font_size)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=font_size)   # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=font_size)   # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=font_size)   # legend fontsize\n",
        "plt.rc('figure', titlesize=font_size)  # fontsize of the figure title\n",
        "plt.figure()\n",
        "for method in [\"reinforce_loo\", \"double_control_variate\", \"disarm\", \"exact_mean_control_variate\", \"reinforce\", \"muprop\"]:\n",
        "    tv, thetas, losses, variances, epsilons, eta_grad_vals, exact_grad_vals, alphas = results[method]    \n",
        "    plt.plot(variances[method], label=method)\n",
        "\n",
        "plt.ylim([0., 1.3e-9])\n",
        "plt.ylabel(\"Gradient Variance\")\n",
        "plt.xlabel(\"Step\")\n",
        "path = \"log_var_toy_D{}_p0{}.pdf\".format(D, target[0][0])\n",
        "plt.savefig(path, dpi=300, bbox_inches='tight')\n",
        "files.download(path) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyD65t9OE4-2"
      },
      "source": [
        "### Objective"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iPPd-kkE7I7"
      },
      "source": [
        "plt.figure()\n",
        "for method in [\"reinforce_loo\", \"double_control_variate\", \"disarm\", \"exact_mean_control_variate\", \"reinforce\", \"muprop\"]:\n",
        "    tv, thetas, losses, variances, epsilons, eta_grad_vals, exact_grad_vals, alphas = results[method]    \n",
        "    plt.plot(losses, label=method)  \n",
        "\n",
        "plt.legend(['RLOO', 'Double CV', 'DisARM', 'R$^*$', 'Double CV', \"Reinforce\", \"MuProp\"])\n",
        "plt.xticks([50, 100, 150, 200], ['500', '1000', '1500', '2000']) \n",
        "print(np.max(losses), np.min(losses))\n",
        "plt.ylabel(\"Average $f(x)$\")\n",
        "plt.xlabel(\"Step\")\n",
        "path = \"loss_toy_D{}_p0{}.pdf\".format(D, target[0][0])\n",
        "plt.savefig(path, dpi=300, bbox_inches='tight')\n",
        "files.download(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul1hw2veNY5S"
      },
      "source": [
        "### Average sigmas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhnA3lmwNPGZ"
      },
      "source": [
        "plt.figure()\n",
        "for method in [\"reinforce_loo\", \"double_control_variate\", \"disarm\", \"exact_mean_control_variate\"]: \n",
        "    tv, thetas, losses, variances, epsilons, eta_grad_vals, exact_grad_vals, alphas = results[method]\n",
        "    print(method, thetas)\n",
        "    plt.plot(thetas, label=method)  \n",
        "\n",
        "plt.xticks([50, 100, 150, 200], ['500', '1000', '1500', '2000']) \n",
        "plt.ylabel(\"Average $\\sigma(\\eta_i)$\")\n",
        "plt.xlabel(\"Step\")\n",
        "path = \"average_mu_D{}_p0{}.pdf\".format(D, target[0][0])\n",
        "plt.savefig(path, dpi=300, bbox_inches='tight')\n",
        "files.download(path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}